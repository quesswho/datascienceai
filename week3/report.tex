\documentclass[a4paper]{article}

\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{biblatex}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{stmaryrd}
\usepackage[dvipsnames]{xcolor}
\usepackage{listings}
\usepackage{float}
\addbibresource{refs.bib}

\begin{document}

\author{
  Sebastian Miles \\
  \href{mailto:miless@chalmers.se}{miless@chalmers.se}
  \and
  Olle Lapidus \\
  \href{mailto:ollelap@chalmers.se}{ollelap@chalmers.se}
}
\title{DAT565/DIT407 Assignment 3}
\date{2024-09-19}

\maketitle
\section*{Problem 1}
Spam messages usually stand out by using aggressive marketing strategies. These emails will frequently use words such as "free", "win", "urgent", etc. They might also contain suspicious links or inquire about your own personal information. In contrast the ham mails will be conversational and more relevant to the receiver. Easy ham is easy to classify due to the straightforward structure and wording. Hard ham contains words and phrasing that might be classified as spam even though it is ham.\\
We used 50\% of the data as training data. With this training we achieved an accuracy of 95.15\%.

88.07


\section*{Problem 2}
Firstly, we began with building onto the code from the previous problem. Secondly, we initialized CountVectorizer from the scikit-learn library. Thirdly, we used fit\_transform and transform on the train set and test set respectively. 

\section*{Problem 3}
Then, we used the BinomialNB function on the transformed train and test set. Thirdly we used the predict function on the 

The performance of the Multinomial Naive Bayes classifier was evaluated using accuracy, precision, recall, and a confusion matrix. The classifier was trained on a dataset containing both spam and ham emails, and the results are as follows:

\subsection*{MultinomialNB}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        & Predicted Ham & Predicted Spam & Sum \\
        \hline
        Actual Ham & 1281 & 1 & 1282 \\
        \hline
        Actual Spam & 73 & 171 & 244 \\
        \hline
        Sum & 1354 & 172 & 1526 \\
        \hline
    \end{tabular}
    \caption{Confusion Matrix for MultinomialNB}
    \label{mNB}
\end{table}


See table \ref{mNB}\\
\textbf{Precision} (ham): 0.95 \\
\textbf{Precision} (spam): 0.99 \\
\textbf{Recall} (ham): 1.00 \\
\textbf{Recall} (spam): 0.70

\subsection*{BernoulliNB}
\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		& Predicted Ham & Predicted Spam & Sum \\
		\hline
		Actual Ham & 1280 & 2 & 1282 \\
		\hline
		Actual Spam & 180 & 64 & 244 \\
		\hline
		Sum & 1460 & 68 & 1528 \\
		\hline
	\end{tabular}
	\caption{Confusion Matrix for BernoulliNB}
	\label{bNB}
\end{table}
\noindent
See table \ref{bNB}\\
\textbf{Precision} (ham): 0.88 \\
\textbf{Precision} (spam): 0.97 \\
\textbf{Recall} (ham): 1.00 \\
\textbf{Recall} (spam): 0.26\\\\
The classifier demonstrates high accuracy, with a precision of 0.99 for spam detection, although the recall for spam is somewhat lower, indicating occasional missed spam emails.

\section*{Problem 4}

\printbibliography
\appendix

\section*{Code}
\label{app:excode}

\lstset{
	language=Python,
	basicstyle=\ttfamily,
	commentstyle=\color{OliveGreen},
	keywordstyle=\bfseries\color{Magenta},
	stringstyle=\color{YellowOrange},
	numbers=left,
	frame=tblr,
	breaklines=true,
	postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\begin{lstlisting}

\end{lstlisting}

\end{document}
